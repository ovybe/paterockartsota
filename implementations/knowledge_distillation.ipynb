{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e72e4e10-7a4a-4370-9c5f-c54bc5a0252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "from opacus import PrivacyEngine\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define data transformations for data augmentation and normalization\n",
    "train_transforms = [\n",
    "        transforms.Resize(size=(180,180)),\n",
    "        transforms.ColorJitter(brightness=0.5,contrast=0.5,saturation=0.5,hue=0.5)\n",
    "        # transforms.RandomRotation(degrees=60),\n",
    "        #transforms.RandomGrayscale(),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "    ]\n",
    "grayscale_transforms = train_transforms.copy()\n",
    "grayscale_transforms.append(transforms.Grayscale(num_output_channels=3))\n",
    "train_transforms_end = [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "]\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose(train_transforms+train_transforms_end),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(size=(180,180)),\n",
    "        #transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5240c359-710f-4cea-8c71-731494fdb3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lemawul/PyTorch\n",
      "/home/lemawul/PyTorch/dataset3\n",
      "[Resize(size=(180, 180), interpolation=bilinear, max_size=None, antialias=True), ColorJitter(brightness=(0.5, 1.5), contrast=(0.5, 1.5), saturation=(0.5, 1.5), hue=(-0.5, 0.5))]\n",
      "2768\n",
      "288\n"
     ]
    }
   ],
   "source": [
    "# Define the data directory\n",
    "ROOT_DIR = os.path.abspath(os.curdir)\n",
    "print(ROOT_DIR)\n",
    "dataset_name='dataset3'\n",
    "data_dir = os.path.join(ROOT_DIR, dataset_name)\n",
    "print(data_dir)\n",
    "\n",
    "concatdatasets = []\n",
    "concatdatasets_val = []\n",
    "# Create data loaders\n",
    "#og_image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), transforms.Compose(train_transforms_end)) for x in ['train']}\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "tr_grayscale_dataset = {x: datasets.ImageFolder(os.path.join(data_dir, x), transforms.Compose(grayscale_transforms+train_transforms_end)) for x in ['train','val']}\n",
    "grayscale_transforms.append(transforms.RandomHorizontalFlip(p=1))\n",
    "tr_grayscale_flipped_dataset = {x: datasets.ImageFolder(os.path.join(data_dir, x), transforms.Compose(grayscale_transforms+train_transforms_end)) for x in ['train','val']}\n",
    "\n",
    "#concatdatasets.append(og_image_datasets['train'])\n",
    "concatdatasets.append(image_datasets['train'])\n",
    "concatdatasets.append(tr_grayscale_dataset['train'])\n",
    "concatdatasets.append(tr_grayscale_flipped_dataset['train'])\n",
    "\n",
    "concatdatasets_val.append(image_datasets['val'])\n",
    "concatdatasets_val.append(tr_grayscale_dataset['val'])\n",
    "concatdatasets_val.append(tr_grayscale_flipped_dataset['val'])\n",
    "\n",
    "r_times = 5;\n",
    "rotate_transf = train_transforms\n",
    "tr_rotate = []\n",
    "print(rotate_transf)\n",
    "for i in range(5):\n",
    "    rotate_transf = train_transforms.copy()\n",
    "    for j in range(i):\n",
    "         rotate_transf.append(transforms.RandomRotation(degrees=(60,60)))\n",
    "    concatdatasets.append(datasets.ImageFolder(os.path.join(data_dir, 'train'), transforms.Compose(rotate_transf+train_transforms_end)))\n",
    "    concatdatasets_val.append(datasets.ImageFolder(os.path.join(data_dir, 'val'), transforms.Compose(rotate_transf+train_transforms_end)))\n",
    "\n",
    "image_datasets['train'] = ConcatDataset(concatdatasets)\n",
    "image_datasets['val'] = ConcatDataset(concatdatasets_val)\n",
    "print(len(image_datasets['train']))\n",
    "print(len(image_datasets['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ea3c2c9-f4ff-40df-8fbd-3bdc6b9eadb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 2768, 'val': 288}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Circle', 'Cross', 'Goat', 'Person', 'Spiral', 'Stag', 'Zigzag']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "print(dataset_sizes)\n",
    "\n",
    "class_names = image_datasets['train'].datasets[0].classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0851a99-fba2-4049-b482-5336900f21bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    criterion = nn.CrossEntropyLoss()  # For teacher validation\n",
    "\n",
    "    with torch.no_grad():  # No gradient computation during validation\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Validation Loss: {loss.item():.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61cbde33-5278-4850-9923-950fb9a2c45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 teacher models.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Directory containing saved teacher models\n",
    "teacher_dir='resnet18'\n",
    "TEACHER_MODELS_DIR = os.path.join(ROOT_DIR, teacher_dir)  # Replace with the actual path\n",
    "\n",
    "# Function to load a teacher model\n",
    "def load_teacher_model(model_path, num_classes=7):\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    num_features = model.fc.in_features\n",
    "    #num_features = model.classifier.in_features\n",
    "    model.fc = nn.Linear(num_features, num_classes)\n",
    "    model.load_state_dict(torch.load(model_path))  # Load model parameters\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    return model\n",
    "\n",
    "# Load all teacher models in the directory\n",
    "teacher_ensemble = []\n",
    "for filename in os.listdir(TEACHER_MODELS_DIR):\n",
    "    if filename.endswith(\".pt\") or filename.endswith(\".pth\"):  # Check for model files\n",
    "        model_path = os.path.join(TEACHER_MODELS_DIR, filename)\n",
    "        teacher_ensemble.append(load_teacher_model(model_path, len(class_names)))\n",
    "\n",
    "print(f\"Loaded {len(teacher_ensemble)} teacher models.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce19d3ed-5dd6-4c56-a8ff-c51dbaab17f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating Teacher 1...\n",
      "Validation Loss: 2.0445, Accuracy: 68.75%\n",
      "Validating Teacher 2...\n",
      "Validation Loss: 1.9918, Accuracy: 62.15%\n",
      "Validating Teacher 3...\n",
      "Validation Loss: 1.9130, Accuracy: 59.03%\n"
     ]
    }
   ],
   "source": [
    "for idx, teacher in enumerate(teacher_ensemble):\n",
    "    print(f\"Validating Teacher {idx + 1}...\")\n",
    "    validate_model(teacher, dataloaders['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "842b1ccc-2917-4864-8212-b0213f1f1e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Logits: 100%|███████████████████████████████████████████████| 87/87 [00:19<00:00,  4.45it/s, Batch Loss=1.78]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 2.0843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Logits: 100%|████████████████████████████████████████████| 87/87 [00:19<00:00,  4.40it/s, Batch Loss=1.53e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.0056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Logits: 100%|███████████████████████████████████████████████| 87/87 [00:20<00:00,  4.33it/s, Batch Loss=1.65]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 2.8317\n",
      "Generated Logits for 261 batches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "teacher_logits = []  # To store teacher predictions\n",
    "\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "def generate_teacher_logits(teacher_model, train_loader):\n",
    "    teacher_model.eval()  # Set to evaluation mode to prevent training operations (e.g., dropout)\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Add a tqdm progress bar for batch progress\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Generating Logits\")\n",
    "\n",
    "    for batch_idx, (X_batch, y_batch) in progress_bar:\n",
    "        # No training, no optimizer\n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            logits = teacher_model(X_batch)  # Generate logits by passing input through the teacher model\n",
    "\n",
    "        # Optionally, apply softmax to get probabilities (for distillation)\n",
    "        softmax = torch.nn.Softmax(dim=1)\n",
    "        teacher_probs = softmax(logits)\n",
    "\n",
    "        # Save logits to the global teacher_logits list\n",
    "        teacher_logits.append(logits.cpu().numpy())  # Convert to numpy and append (for later use in distillation or analysis)\n",
    "\n",
    "        # Optionally compute and track the loss (monitoring purposes only)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(logits, y_batch)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Update tqdm progress bar description with batch loss (optional)\n",
    "        progress_bar.set_postfix({\"Batch Loss\": loss.item()})\n",
    "\n",
    "    # If you tracked loss, you can print average loss\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "# Example of calling the function for each teacher in the ensemble\n",
    "for teacher in teacher_ensemble:\n",
    "    # We don't need to retrain the teacher, just generate logits\n",
    "    teacher.eval()  # Set the teacher model to evaluation mode\n",
    "    generate_teacher_logits(teacher, dataloaders['train'])\n",
    "\n",
    "# After running this, teacher_logits will contain logits from all batches across all teachers in the ensemble.\n",
    "print(f\"Generated Logits for {len(teacher_logits)} batches.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1bdcb97-e704-4f6e-8b16-39e4f289fb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Noisy Logits of size: torch.Size([2768, 7])\n"
     ]
    }
   ],
   "source": [
    "# Aggregate noisy logits with smaller batches and CPU memory optimization\n",
    "def aggregate_teacher_logits(teacher_ensemble, data_loader, noise_scale=0.1, class_num=7):\n",
    "    logits_list = []\n",
    "\n",
    "    for X_batch, _ in data_loader:\n",
    "        batch_logits = torch.zeros(len(X_batch), class_num).cpu()  # Move to CPU\n",
    "        for teacher in teacher_ensemble:\n",
    "            # Ensure no gradients are computed and move to CPU\n",
    "            with torch.no_grad():\n",
    "                logits = teacher(X_batch.cpu())  # Move inputs to CPU if they're on GPU\n",
    "                batch_logits += logits\n",
    "        batch_logits /= len(teacher_ensemble)  # Average logits\n",
    "        noisy_logits = batch_logits + torch.normal(0, noise_scale, batch_logits.shape).cpu()  # Add DP noise\n",
    "        logits_list.append(noisy_logits)\n",
    "\n",
    "        # Optional: clear cache after each batch to free up memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return torch.cat(logits_list)\n",
    "\n",
    "# Ensure smaller batch sizes to avoid OOM\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "# Generate noisy logits from the teacher ensemble\n",
    "teacher_logits_noisy = aggregate_teacher_logits(teacher_ensemble, dataloaders['train'], noise_scale=0.1, class_num=len(class_names))\n",
    "\n",
    "# Check size of noisy logits\n",
    "print(f\"Generated Noisy Logits of size: {teacher_logits_noisy.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf7ca8f9-0144-42e1-a12f-2ff4de430bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model = models.resnet18(pretrained=True)\n",
    "num_features = student_model.fc.in_features\n",
    "student_model.fc = nn.Linear(num_features, len(class_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7fb73c23-4544-413e-8d0b-e855f70ba5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def replace_batchnorm_with_groupnorm(model):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, nn.BatchNorm2d):  # BatchNorm2d example\n",
    "            # Create GroupNorm layer with appropriate num_groups (typically batch size or smaller)\n",
    "            num_features = module.num_features\n",
    "            num_groups = 32  # You can adjust this value as needed\n",
    "            group_norm = nn.GroupNorm(num_groups, num_features)\n",
    "            setattr(model, name, group_norm)\n",
    "        else:\n",
    "            replace_batchnorm_with_groupnorm(module)\n",
    "    return model\n",
    "\n",
    "# Example of replacing BatchNorm with GroupNorm in your student model\n",
    "student_model = replace_batchnorm_with_groupnorm(student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ed18b36-fa9f-472a-a394-abcf176414b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print(dataloaders['train'].batch_size)  # This should give you a valid integer batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aba45221-99c1-4004-b024-f99866b0cdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_student_with_validation(student_model, teacher_logits, train_loader, val_loader, epsilon, delta):\n",
    "    student_optimizer = optim.SGD(student_model.parameters(), lr=0.01)\n",
    "    criterion = nn.KLDivLoss(reduction=\"batchmean\")  # KL Divergence for distillation loss\n",
    "    privacy_engine = PrivacyEngine()\n",
    "\n",
    "    # Add differential privacy to the student model\n",
    "    student_model, student_optimizer, train_loader = privacy_engine.make_private(\n",
    "        module=student_model,\n",
    "        optimizer=student_optimizer,\n",
    "        data_loader=train_loader,\n",
    "        noise_multiplier=1.0,  # Set noise multiplier for DP\n",
    "        max_grad_norm=1.0,    # Clip gradients\n",
    "    )\n",
    "\n",
    "    for epoch in range(100):  # Train for 100 epochs\n",
    "        student_model.train()  # Set the model to training mode\n",
    "        batch_size = train_loader.batch_size  # Ensure batch_size is defined\n",
    "        for i, (X_batch, _) in enumerate(train_loader):\n",
    "            student_optimizer.zero_grad()\n",
    "\n",
    "            # Get teacher's noisy logits for this batch\n",
    "            target_logits = teacher_logits[i * batch_size:(i + 1) * batch_size]\n",
    "\n",
    "            # Forward pass through the student\n",
    "            student_outputs = student_model(X_batch)\n",
    "\n",
    "            # Compute distillation loss\n",
    "            loss = criterion(\n",
    "                nn.functional.log_softmax(student_outputs, dim=1),\n",
    "                nn.functional.softmax(target_logits, dim=1)\n",
    "            )\n",
    "            loss.backward()\n",
    "            student_optimizer.step()\n",
    "\n",
    "        # Validate the student model after every epoch\n",
    "        print(f\"Epoch {epoch + 1} Training Loss: {loss.item():.4f}\")\n",
    "        print(f\"Validating Student Model at Epoch {epoch + 1}...\")\n",
    "        validate_model(student_model, val_loader)\n",
    "\n",
    "    # Privacy accounting\n",
    "    epsilon, delta = privacy_engine.get_epsilon(delta)\n",
    "    print(f\"Model trained with (ε = {epsilon:.2f}, δ = {delta}) differential privacy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59c3b99a-e0a1-4eaa-8bc2-e648ca49d6ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 44\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# def train_student_with_validation(student_model, teacher_logits, train_loader, val_loader, epsilon, delta):\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#     student_optimizer = optim.SGD(student_model.parameters(), lr=0.01)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#     criterion = nn.KLDivLoss(reduction=\"batchmean\")  # KL Divergence for distillation loss\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Train the student model\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[43mtrain_student_with_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudent_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_logits_noisy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[40], line 22\u001b[0m, in \u001b[0;36mtrain_student_with_validation\u001b[0;34m(student_model, teacher_logits, train_loader, val_loader, epsilon, delta)\u001b[0m\n\u001b[1;32m     19\u001b[0m student_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Get teacher's noisy logits for this batch\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m target_logits \u001b[38;5;241m=\u001b[39m teacher_logits[\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m:(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m batch_size]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Forward pass through the student\u001b[39;00m\n\u001b[1;32m     25\u001b[0m student_outputs \u001b[38;5;241m=\u001b[39m student_model(X_batch)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# def train_student_with_validation(student_model, teacher_logits, train_loader, val_loader, epsilon, delta):\n",
    "#     student_optimizer = optim.SGD(student_model.parameters(), lr=0.01)\n",
    "#     criterion = nn.KLDivLoss(reduction=\"batchmean\")  # KL Divergence for distillation loss\n",
    "#     privacy_engine = PrivacyEngine()\n",
    "\n",
    "#     # Add differential privacy to the student model\n",
    "#     student_model, student_optimizer, train_loader = privacy_engine.make_private(\n",
    "#         module=student_model,\n",
    "#         optimizer=student_optimizer,\n",
    "#         data_loader=train_loader,\n",
    "#         noise_multiplier=1.0,  # Set noise multiplier for DP\n",
    "#         max_grad_norm=1.0,    # Clip gradients\n",
    "#     )\n",
    "\n",
    "#     for epoch in range(100):  # Train for 100 epochs\n",
    "#         student_model.train()  # Set the model to training mode\n",
    "#         for i, (X_batch, _) in enumerate(train_loader):\n",
    "#             student_optimizer.zero_grad()\n",
    "\n",
    "#             # Get teacher's noisy logits for this batch\n",
    "#             target_logits = teacher_logits[i * train_loader.batch_size:(i + 1) * train_loader.batch_size]\n",
    "\n",
    "#             # Forward pass through the student\n",
    "#             student_outputs = student_model(X_batch)\n",
    "\n",
    "#             # Compute distillation loss\n",
    "#             loss = criterion(\n",
    "#                 nn.functional.log_softmax(student_outputs, dim=1),\n",
    "#                 nn.functional.softmax(target_logits, dim=1)\n",
    "#             )\n",
    "#             loss.backward()\n",
    "#             student_optimizer.step()\n",
    "\n",
    "#         # Validate the student model after every epoch\n",
    "#         print(f\"Epoch {epoch + 1} Training Loss: {loss.item():.4f}\")\n",
    "#         print(f\"Validating Student Model at Epoch {epoch + 1}...\")\n",
    "#         validate_model(student_model, val_loader)\n",
    "\n",
    "#     # Privacy accounting\n",
    "#     epsilon, delta = privacy_engine.get_epsilon(delta)\n",
    "#     print(f\"Model trained with (ε = {epsilon:.2f}, δ = {delta}) differential privacy\")\n",
    "\n",
    "# Train the student model\n",
    "train_student_with_validation(student_model, teacher_logits_noisy, dataloaders['train'], dataloaders['val'], epsilon=3.0, delta=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c6363d-02e1-42fa-b2e5-4e5285e1f5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(student_model.state_dict(), f\"student_model_opacus_{teacher_dir}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f176a3-b3ae-4688-b9b2-648f4d21686f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
